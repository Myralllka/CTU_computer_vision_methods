{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Matching, RANSAC and integration\n",
    "This is a notebook, which could help you with testing fourth lab assignment.\n",
    "It contains utility functions for visualization, some test input for the functions you needs to implement,\n",
    "and the output of the reference solution for the same test input.\n",
    "\n",
    "template functions for the assignment contain a short description of what the function is supposed to do,\n",
    "and produce an incorrect output, which is nevertheless in proper format: type and shape.\n",
    "\n",
    "You are not allowed to use kornia or opencv or any other library functions, which are specifically designed\n",
    "to perform the operations requested in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia\n",
    "import cv2\n",
    "\n",
    "\n",
    "def plot_torch(x, y, *kwargs):\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy(), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch(tensor,figsize=(8,6), *kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(kornia.tensor_to_image(tensor), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch_channels(tensor, dim = 1, *kwargs):\n",
    "    num_ch = tensor.size(dim)\n",
    "    fig=plt.figure(figsize=(num_ch*5,5))\n",
    "    tensor_splitted = torch.split(tensor, 1, dim=dim)\n",
    "    for i in range(num_ch):\n",
    "        fig.add_subplot(1, num_ch, i+1)\n",
    "        plt.imshow(kornia.tensor_to_image(tensor_splitted[i].squeeze(dim)), *kwargs)\n",
    "    return\n",
    "\n",
    "def timg_load(fname, to_gray = True):\n",
    "    img = cv2.imread(fname)\n",
    "    with torch.no_grad():\n",
    "        timg = kornia.image_to_tensor(img, False).float()\n",
    "        if to_gray:\n",
    "            timg = kornia.color.bgr_to_grayscale(timg)\n",
    "        else:\n",
    "            timg = kornia.color.bgr_to_rgb(timg)\n",
    "    return timg\n",
    "\n",
    "\n",
    "def keypoint_locations_to_opencv_kps(keypoint_locations, increase_scale=1.0):\n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4].item(),\n",
    "                         b_ch_sc_y_x[3].item(),\n",
    "                         b_ch_sc_y_x[2].item()*increase_scale)\n",
    "            for b_ch_sc_y_x in keypoint_locations if b_ch_sc_y_x[0].item() == 0]\n",
    "    return kpts\n",
    "\n",
    "def visualize_detections(img, keypoint_locations, img_idx = 0, increase_scale = 1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = keypoint_locations_to_opencv_kps(keypoint_locations, increase_scale)\n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(kornia.tensor_to_image(img).astype(np.uint8),\n",
    "                                kpts,\n",
    "                                vis_img, \n",
    "                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(vis_img)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = torch.tensor([[0,1],[1,1], [-1,1], [0,0.5]]).view(-1,2).float()\n",
    "v2 = torch.cat([torch.tensor([[3,3.],[5,5.]]),v1, torch.tensor([[2.,2.]])], dim=0)\n",
    "print (v1)\n",
    "print (v2)\n",
    "desc1=v1\n",
    "desc2=v2\n",
    "distance_matrix = torch.cdist(desc1,desc2)\n",
    "imshow_torch(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matching import *\n",
    "\n",
    "match_idxs, vals = match_nn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_mnn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_smnn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from matching import *\n",
    "\n",
    "\n",
    "match_idxs, vals = match_nn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_mnn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_smnn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "```\n",
    "    tensor([[0, 2],\n",
    "            [1, 3],\n",
    "            [2, 4],\n",
    "            [3, 5]]) tensor([0., 0., 0., 0.])\n",
    "    tensor([[0, 2],\n",
    "            [1, 3],\n",
    "            [2, 4],\n",
    "            [3, 5]]) tensor([0., 0., 0., 0.])\n",
    "    tensor([[0, 2],\n",
    "            [1, 3],\n",
    "            [2, 4],\n",
    "            [3, 5]]) tensor([0., 0., 0., 0.])\n",
    "    tensor([[0, 2],\n",
    "            [1, 3],\n",
    "            [2, 4],\n",
    "            [3, 5]]) tensor([0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And other direction\n",
    "\n",
    "from matching import *\n",
    "\n",
    "desc1=v2\n",
    "desc2=v1\n",
    "\n",
    "match_idxs, vals = match_nn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_mnn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_smnn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "#And other direction\n",
    "\n",
    "from matching import *\n",
    "\n",
    "desc1=v2\n",
    "desc2=v1\n",
    "\n",
    "match_idxs, vals = match_nn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_mnn(desc1, desc2)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "match_idxs, vals = match_smnn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "```\n",
    "    tensor([[0, 1],\n",
    "            [1, 1],\n",
    "            [2, 0],\n",
    "            [3, 1],\n",
    "            [4, 2],\n",
    "            [5, 3],\n",
    "            [6, 1]]) tensor([2.8284, 5.6569, 0.0000, 0.0000, 0.0000, 0.0000, 1.4142])\n",
    "    tensor([[2, 0],\n",
    "            [3, 1],\n",
    "            [4, 2],\n",
    "            [5, 3]]) tensor([0., 0., 0., 0.])\n",
    "    tensor([[0, 1],\n",
    "            [2, 0],\n",
    "            [3, 1],\n",
    "            [4, 2],\n",
    "            [5, 3],\n",
    "            [6, 1]]) tensor([0.7845, 0.0000, 0.0000, 0.0000, 0.0000, 0.6325])\n",
    "    tensor([[2, 0],\n",
    "            [3, 1],\n",
    "            [4, 2],\n",
    "            [5, 3]]) tensor([0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_describe(img,\n",
    "                        det='hessian',\n",
    "                        th=0.0005,\n",
    "                        affine=True,\n",
    "                        PS = 41):\n",
    "    if det.lower() == 'hessian':\n",
    "        keypoint_locations = scalespace_hessian(img, th)\n",
    "    elif det.lower() == 'harris':\n",
    "        keypoint_locations = scalespace_harris(img, th)\n",
    "    else:\n",
    "        raise ValueError('Unknown detector, try hessian or harris')\n",
    "    n_kp = keypoint_locations.size(0)\n",
    "    A, img_idxs = affine_from_location(keypoint_locations)\n",
    "    if affine:\n",
    "        patches  = extract_affine_patches(img, A, img_idxs, 19, 3.0)\n",
    "        aff_shape = estimate_patch_affine_shape(patches)\n",
    "        dummy_angles = torch.zeros(n_kp,1, dtype=torch.float, device=img.device)\n",
    "                                                          \n",
    "        A, img_idxs = affine_from_location_and_orientation_and_affshape(keypoint_locations, \n",
    "                                                          dummy_angles,\n",
    "                                                          aff_shape)\n",
    "    patches =  extract_affine_patches(img, A, img_idxs, 19, 3.0)\n",
    "    ori = estimate_patch_dominant_orientation(patches)\n",
    "    if affine:\n",
    "        A, img_idxs = affine_from_location_and_orientation_and_affshape(keypoint_locations, \n",
    "                                                  ori,\n",
    "                                                  aff_shape)\n",
    "    else:\n",
    "        A, img_idxs = affine_from_location_and_orientation(keypoint_locations, \n",
    "                                                  ori)\n",
    "    patches =  extract_affine_patches(img, A, img_idxs, PS, 5.0)\n",
    "    descs = calc_sift_descriptor(patches)\n",
    "    return keypoint_locations, descs, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_snn(descs1, descs2, 0.7)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tentatives_to_opencv(match_idxs, vals):\n",
    "    tentative_matches = [cv2.DMatch(i[0].item(), i[1].item(), 0, vals[idx].item()) for idx,i in enumerate(match_idxs)]\n",
    "    return tentative_matches\n",
    "    \n",
    "kps1  = keypoint_locations_to_opencv_kps(keypoint_locations1)\n",
    "kps2  = keypoint_locations_to_opencv_kps(keypoint_locations2)\n",
    "\n",
    "match_idxs, vals = match_smnn(descs1, descs2, 0.8)\n",
    "tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "\n",
    "\n",
    "img_matches = np.empty((max(timg1.shape[2], timg2.shape[2]), \n",
    "                        timg1.shape[3]+timg2.shape[3], 3), dtype=np.uint8)\n",
    "img1 = (kornia.tensor_to_image(timg1)*255.).astype(np.uint8)\n",
    "img2 = (kornia.tensor_to_image(timg2)*255.).astype(np.uint8)\n",
    "\n",
    "cv2.drawMatches(img1, kps1,\n",
    "                img2, kps2,\n",
    "                tentative_matches, img_matches, \n",
    "                flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reference example\n",
    "```python\n",
    "def tentatives_to_opencv(match_idxs, vals):\n",
    "    tentative_matches = [cv2.DMatch(i[0].item(), i[1].item(), vals[idx]) for idx,i in enumerate(match_idxs)]\n",
    "    return tentative_matches\n",
    "    \n",
    "kps1  = keypoint_locations_to_opencv_kps(keypoint_locations1)\n",
    "kps2  = keypoint_locations_to_opencv_kps(keypoint_locations2)\n",
    "\n",
    "match_idxs, vals = match_snn(descs1, descs2, 0.8)\n",
    "tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "\n",
    "\n",
    "img_matches = np.empty((max(timg1.shape[2], timg2.shape[2]), \n",
    "                        timg1.shape[3]+timg2.shape[3], 3), dtype=np.uint8)\n",
    "img1 = (kornia.tensor_to_image(timg1)*255.).astype(np.uint8)\n",
    "img2 = (kornia.tensor_to_image(timg2)*255.).astype(np.uint8)\n",
    "\n",
    "cv2.drawMatches(img1, kps1,\n",
    "                img2, kps2,\n",
    "                tentative_matches, img_matches, \n",
    "                flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_matches)\n",
    "```\n",
    "\n",
    "![image.png](matching_and_ransac_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decolorize(img):\n",
    "    return  cv2.cvtColor(cv2.cvtColor(img,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "def draw_matches(kps1, kps2, tentative_matches, H,  H_gt, inlier_mask, img1, img2):\n",
    "    matchesMask = inlier_mask.ravel().tolist()\n",
    "    h,w, ch = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    #Ground truth transformation\n",
    "    dst_GT = cv2.perspectiveTransform(pts, H_gt)\n",
    "    img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)\n",
    "    img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "    # Blue is estimated, green is ground truth homography\n",
    "    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 20)\n",
    "    img_out = cv2.drawMatches(decolorize(img1),kps1,img2_tr,kps2,tentative_matches,None,**draw_params)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img_out)\n",
    "    return\n",
    "H_gt = np.loadtxt('v_woman_H_1_6')\n",
    "#Geometric verification (RANSAC)\n",
    "from copy import deepcopy\n",
    "def verify(tentative_matches, kps1, kps2):\n",
    "    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,1.0)\n",
    "    return H, inlier_mask\n",
    "    \n",
    "\n",
    "H, inliers =  verify(tentative_matches, kps1, kps2)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches, H, H_gt, inliers, cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "              cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "def decolorize(img):\n",
    "    return  cv2.cvtColor(cv2.cvtColor(img,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "def draw_matches(kps1, kps2, tentative_matches, H,  H_gt, inlier_mask, img1, img2):\n",
    "    matchesMask = inlier_mask.ravel().tolist()\n",
    "    h,w, ch = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    #Ground truth transformation\n",
    "    dst_GT = cv2.perspectiveTransform(pts, H_gt)\n",
    "    img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)\n",
    "    img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "    # Blue is estimated, green is ground truth homography\n",
    "    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 20)\n",
    "    img_out = cv2.drawMatches(decolorize(img1),kps1,img2_tr,kps2,tentative_matches,None,**draw_params)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img_out)\n",
    "    return\n",
    "H_gt = np.loadtxt('v_woman_H_1_6')\n",
    "#Geometric verification (RANSAC)\n",
    "from copy import deepcopy\n",
    "def verify(tentative_matches, kps1, kps2):\n",
    "    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,1.0)\n",
    "    return H, inlier_mask\n",
    "    \n",
    "\n",
    "H, inliers =  verify(tentative_matches, kps1, kps2)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches, H, H_gt, inliers, cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "              cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))\n",
    "```\n",
    "![image.png](matching_and_ransac_files/att_00001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_smnn(descs1, descs2, 0.85) \n",
    "    tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1)\n",
    "    H, inl = ransac_h(pts_matches, 4.0, 0.99, 10000)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches,\n",
    "             H.detach().cpu().numpy(),\n",
    "             H_gt,\n",
    "             inl.cpu().numpy(),\n",
    "             cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "             cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference solution \n",
    "\n",
    "```python\n",
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_smnn(descs1, descs2, 0.85) \n",
    "    tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1)\n",
    "    H, inl = ransac_h(pts_matches, 4.0, 0.99, 10000)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches,\n",
    "             H.detach().cpu().numpy(),\n",
    "             H_gt,\n",
    "             inl.cpu().numpy(),\n",
    "             cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "             cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))\n",
    "```\n",
    "![image.png](matching_and_ransac_files/att_00002.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}