{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Local feature detection\n",
    "This is a notebook, which could help you with testing second lab assignment.\n",
    "It contains utility functions for visualization, some test input for the functions you needs to implement,\n",
    "and the output of the reference solution for the same test input.\n",
    "\n",
    "template functions for the assignment contain a short description of what the function is supposed to do,\n",
    "and produce an incorrect output, which is nevertheless in proper format: type and shape.\n",
    "\n",
    "You are not allowed to use kornia or opencv or any other library functions, which are specifically designed\n",
    "to perform the operations requested in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# % load_ext autoreload\n",
    "# % autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Visualization functions\n",
    "def plot_torch(x, y, *kwargs):\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy(), *kwargs)\n",
    "    return\n",
    "\n",
    "\n",
    "def imshow_torch(tensor, figsize=(8, 6), *kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(kornia.tensor_to_image(tensor), *kwargs)\n",
    "    return\n",
    "\n",
    "\n",
    "def imshow_torch_channels(tensor, dim=1, *kwargs):\n",
    "    num_ch = tensor.size(dim)\n",
    "    fig = plt.figure(figsize=(num_ch * 5, 5))\n",
    "    tensor_splitted = torch.split(tensor, 1, dim=dim)\n",
    "    for i in range(num_ch):\n",
    "        fig.add_subplot(1, num_ch, i + 1)\n",
    "        plt.imshow(kornia.tensor_to_image(tensor_splitted[i].squeeze(dim)),\n",
    "                   *kwargs)\n",
    "    return\n",
    "\n",
    "\n",
    "def timg_load(fname, to_gray=True):\n",
    "    img = cv2.imread(fname)\n",
    "    with torch.no_grad():\n",
    "        timg = kornia.image_to_tensor(img, False).float()\n",
    "        if to_gray:\n",
    "            timg = kornia.color.bgr_to_grayscale(timg)\n",
    "        else:\n",
    "            timg = kornia.color.bgr_to_rgb(timg)\n",
    "    return timg\n",
    "\n",
    "\n",
    "img_blobs = timg_load('sin.png')\n",
    "\n",
    "# Source https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_corner.html\n",
    "img_corners = timg_load('corners.png')\n",
    "\n",
    "imshow_torch(img_blobs)\n",
    "imshow_torch(img_corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from local_detector import hessian_response\n",
    "\n",
    "resp_small = hessian_response(img_blobs, 1.6)\n",
    "resp_big = hessian_response(img_blobs, 25.)\n",
    "\n",
    "imshow_torch_channels(torch.cat([resp_small, resp_big], dim=0), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from local_detector import hessian_response\n",
    "\n",
    "resp_small = hessian_response(img_blobs, 1.6)\n",
    "resp_big = hessian_response(img_blobs, 25.)\n",
    "\n",
    "imshow_torch_channels(torch.cat([resp_small,\n",
    "                                 resp_big], dim=0), 0)\n",
    "```\n",
    "![image.png](local_detector_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_detector import harris_response\n",
    "\n",
    "resp_small = harris_response(img_corners, 1.6, 2.0, 0.04)\n",
    "resp_big = harris_response(img_corners, 7., 9., 0.04)\n",
    "\n",
    "imshow_torch_channels(torch.cat([resp_small,\n",
    "                                 resp_big], dim=0), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from local_detector import harris_response\n",
    "\n",
    "resp_small = harris_response(img_corners, 1.6, 2.0, 0.04)\n",
    "resp_big = harris_response(img_corners, 7., 9., 0.04)\n",
    "\n",
    "imshow_torch_channels(torch.cat([resp_small, \n",
    "                                 resp_big], dim=0), 0)\n",
    "```\n",
    "![image.png](local_detector_files/att_00001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_detector import nms2d\n",
    "    \n",
    "nmsed_harris = nms2d(resp_small, 0.00004)\n",
    "imshow_torch(nmsed_harris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from local_detector import nms2d\n",
    "\n",
    "nmsed_harris = nms2d(resp_small, 0.00004)\n",
    "\n",
    "imshow_torch(nmsed_harris)\n",
    "```\n",
    "![image.png](local_detector_files/att_00002.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_detector import harris\n",
    "\n",
    "keypoint_locations = harris(img_corners, 1.6, 2.0, 0.00001)\n",
    "print(keypoint_locations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from local_detector import harris\n",
    "\n",
    "keypoint_locations = harris(img_corners, 1.6, 2.0, 0.00001)\n",
    "print (keypoint_locations)\n",
    "```\n",
    "\n",
    "    tensor([[  0,   0,  21,  32],\n",
    "            [  0,   0,  24,  55],\n",
    "            [  0,   0,  28,  86],\n",
    "            [  0,   0,  31, 110],\n",
    "            [  0,   0,  31, 201],\n",
    "            [  0,   0,  31, 248],\n",
    "            [  0,   0,  44,  42],\n",
    "            [  0,   0,  46,  33],\n",
    "            [  0,   0,  48,  49],\n",
    "            [  0,   0,  48,  70],\n",
    "            [  0,   0,  51,  97],\n",
    "            [  0,   0,  52,  77],\n",
    "            [  0,   0,  55, 124],\n",
    "            [  0,   0,  56, 104],\n",
    "            [  0,   0,  70,  48],\n",
    "            [  0,   0,  71,  60],\n",
    "            [  0,   0,  75,  87],\n",
    "            [  0,   0,  76,  67],\n",
    "            [  0,   0,  78, 201],\n",
    "            [  0,   0,  79,  94],\n",
    "            [  0,   0,  79, 115],\n",
    "            [  0,   0,  79, 249],\n",
    "            [  0,   0,  81, 298],\n",
    "            [  0,   0,  83, 122],\n",
    "            [  0,   0,  86, 144],\n",
    "            [  0,   0,  99,  78],\n",
    "            [  0,   0, 101,  68],\n",
    "            [  0,   0, 102, 105],\n",
    "            [  0,   0, 103,  85],\n",
    "            [  0,   0, 106, 132],\n",
    "            [  0,   0, 107, 112],\n",
    "            [  0,   0, 109, 159],\n",
    "            [  0,   0, 110, 139],\n",
    "            [  0,   0, 114,  76],\n",
    "            [  0,   0, 115,  88],\n",
    "            [  0,   0, 120, 120],\n",
    "            [  0,   0, 122, 143],\n",
    "            [  0,   0, 128, 251],\n",
    "            [  0,   0, 128, 298],\n",
    "            [  0,   0, 160,  78],\n",
    "            [  0,   0, 160, 272]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_detector import create_scalespace\n",
    "# from imagefiltering import gaussian_filter2d\n",
    "\n",
    "pyr, sigmas = create_scalespace(img_blobs, 10, 1.6)\n",
    "\n",
    "resps = torch.stack([hessian_response(x.squeeze(2), 3.6) for x in torch.split(pyr, 1, 2)], dim=2)\n",
    "\n",
    "imshow_torch_channels(pyr, 2)\n",
    "imshow_torch_channels(resps, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from local_detector import create_scalespace\n",
    "pyr, sigmas = create_scalespace(img_blobs, 10, 1.6)\n",
    "print (pyr.shape)\n",
    "resps = torch.stack([hessian_response(x.squeeze(2), 3.6) for x in torch.split(pyr,1,2)], dim=2)\n",
    "imshow_torch_channels(pyr, 2)\n",
    "imshow_torch_channels(resps, 2)\n",
    "```\n",
    "    \n",
    "    torch.Size([1, 1, 10, 256, 256])\n",
    "\n",
    "![image.png](local_detector_files/att_00003.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_detector import create_scalespace\n",
    "pyr, sigmas = create_scalespace(img_corners, 10, 1.3)\n",
    "print (pyr.shape)\n",
    "resps = torch.stack([harris_response(x.squeeze(2), 3.6, 4.0, 0.04) for x in torch.split(pyr,1,2)], dim=2)\n",
    "imshow_torch_channels(pyr, 2)\n",
    "imshow_torch_channels(resps, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference solution\n",
    "\n",
    "```python\n",
    "from local_detector import create_scalespace\n",
    "pyr, sigmas = create_scalespace(img_corners, 10, 1.3)\n",
    "print (pyr.shape) \n",
    "resps = torch.stack([harris_response(x.squeeze(2), 3.6, 4.0, 0.04) for x in torch.split(pyr,1,2)], dim=2)\n",
    "imshow_torch_channels(pyr, 2)\n",
    "imshow_torch_channels(resps, 2)\n",
    "```\n",
    "\n",
    "![image.png](local_detector_files/att_00004.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_detector import scalespace_hessian\n",
    "from local_detector import *\n",
    "\n",
    "def visualize_detections(img, keypoint_locations, img_idx=0,\n",
    "                         increase_scale=1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4].item(),\n",
    "                         b_ch_sc_y_x[3].item(),\n",
    "                         b_ch_sc_y_x[2].item())\n",
    "            for b_ch_sc_y_x in keypoint_locations if\n",
    "            b_ch_sc_y_x[0].item() == img_idx]\n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(\n",
    "            kornia.tensor_to_image(img).astype(np.uint8),\n",
    "            kpts,\n",
    "            vis_img,\n",
    "            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(vis_img)\n",
    "    return\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations = scalespace_hessian(img_blobs / img_blobs.max(), 0.003)\n",
    "    \n",
    "visualize_detections(img_blobs, keypoint_locations, increase_scale=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from local_detector import scalespace_hessian\n",
    "def visualize_detections(img, keypoint_locations, img_idx = 0, increase_scale = 1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4],b_ch_sc_y_x[3],b_ch_sc_y_x[2])\n",
    "            for b_ch_sc_y_x in keypoint_locations if b_ch_sc_y_x[0] == img_idx]\n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(kornia.tensor_to_image(img).astype(np.uint8),\n",
    "                                kpts,\n",
    "                                vis_img, \n",
    "                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(vis_img)\n",
    "    return\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations = scalespace_hessian(img_blobs/img_blobs.max(), 0.003)\n",
    "\n",
    "visualize_detections(img_blobs, keypoint_locations, increase_scale=1.0)\n",
    "```\n",
    "![image.png](local_detector_files/att_00005.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_detector import scalespace_hessian\n",
    "\n",
    "\n",
    "def visualize_detections(img, keypoint_locations, img_idx=0,\n",
    "                         increase_scale=1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4].item(),\n",
    "                         b_ch_sc_y_x[3].item(),\n",
    "                         increase_scale * b_ch_sc_y_x[2].item())\n",
    "            for b_ch_sc_y_x in keypoint_locations if\n",
    "            b_ch_sc_y_x[0].item() == img_idx]\n",
    "\n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(\n",
    "            kornia.tensor_to_image(img).astype(np.uint8),\n",
    "            kpts,\n",
    "            vis_img,\n",
    "            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(vis_img)\n",
    "    return\n",
    "\n",
    "\n",
    "img_sun = timg_load('sunflowers.png', True)\n",
    "with torch.no_grad():\n",
    "    keypoint_locations = scalespace_hessian(img_sun / img_sun.max(), 0.02)\n",
    "\n",
    "visualize_detections(img_sun, keypoint_locations, increase_scale=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from local_detector import scalespace_hessian\n",
    "def visualize_detections(img, keypoint_locations, img_idx = 0, increase_scale = 1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4],b_ch_sc_y_x[3],increase_scale*b_ch_sc_y_x[2])\n",
    "            for b_ch_sc_y_x in keypoint_locations if b_ch_sc_y_x[0] == img_idx]\n",
    "    \n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(kornia.tensor_to_image(img).astype(np.uint8),\n",
    "                                kpts,\n",
    "                                vis_img, \n",
    "                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(vis_img)\n",
    "    return\n",
    "\n",
    "img_sun = timg_load('sunflowers.png', True)\n",
    "with torch.no_grad():\n",
    "    keypoint_locations = scalespace_hessian(img_sun/img_sun.max(), 0.02)\n",
    "\n",
    "visualize_detections(img_sun, keypoint_locations, increase_scale=3.0)\n",
    "```\n",
    "![image.png](local_detector_files/att_00006.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_detector import scalespace_harris\n",
    "from local_detector import harris_response\n",
    "from imagefiltering import gaussian_filter2d\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations = scalespace_harris(img_corners, 0.00001)\n",
    "\n",
    "visualize_detections(img_corners * 255., keypoint_locations,\n",
    "                     increase_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from local_detector import scalespace_harris\n",
    "from imagefiltering import gaussian_filter2d\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations = scalespace_harris(img_corners, 0.00001)\n",
    "\n",
    "visualize_detections(img_corners*255., keypoint_locations, increase_scale=1.0)\n",
    "```\n",
    "![image.png](local_detector_files/att_00007.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
