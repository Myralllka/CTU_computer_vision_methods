{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: introduction into image filtering using PyTorch\n",
    "This is a notebook, which could help you with testing first lab assignment.\n",
    "It contains utility functions for visualization, some test input for the functions you needs to implement,\n",
    "and the output of the reference solution for the same test input.\n",
    "\n",
    "template functions for the assignment contain a short description of what the function is supposed to do,\n",
    "and produce an incorrect output, which is nevertheless in proper format: type and shape.\n",
    "\n",
    "You are not allowed to use kornia or opencv or any other library functions, which are specifically designed\n",
    "to perform the operations requested in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia\n",
    "\n",
    "\n",
    "def plot_torch(x, y, *kwargs):\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy(), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch(tensor, *kwargs):\n",
    "    plt.figure()\n",
    "    plt.imshow(kornia.tensor_to_image(tensor), *kwargs)\n",
    "    return\n",
    "inp = torch.linspace(-12, 12, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagefiltering import gaussian1d\n",
    "plot_torch(inp, gaussian1d(inp, 3.0), 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "```python\n",
    "from imagefiltering import gaussian1d\n",
    "plot_torch(inp, gaussian1d(inp, 3.0), 'g-')\n",
    "\n",
    "```\n",
    "\n",
    "![image.png](imagefiltering_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagefiltering import gaussian_deriv1d\n",
    "plot_torch(inp, gaussian_deriv1d(inp, 3.0), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from lab0_reference.imagefiltering import gaussian_deriv1d\n",
    "plot_torch(inp, gaussian_deriv1d(inp, 3.0), 'r-')\n",
    "```\n",
    "\n",
    "![image.png](imagefiltering_files/att_00001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from imagefiltering import filter2d #, dgauss, gaussfilter, gaussderiv, gaussderiv2\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,16,16] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "kernel = torch.ones(3,3)\n",
    "out = filter2d(inp, kernel)\n",
    "imshow_torch(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor with initial value## Reference example\n",
    "```python\n",
    "from imagefiltering import filter2d\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,16,16] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "kernel = torch.ones(3,3)\n",
    "\n",
    "out = filter2d(inp, kernel)\n",
    "imshow_torch(out)\n",
    "```\n",
    "![image.png](imagefiltering_files/att_00002.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagefiltering import gaussian_filter2d\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,15,15] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "sigma = 3.0\n",
    "out = gaussian_filter2d(inp, sigma)\n",
    "imshow_torch(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from lab0_reference.imagefiltering import gaussian_filter2d\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,15,15] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "sigma = 3.0\n",
    "out = gaussian_filter2d(inp, sigma)\n",
    "imshow_torch(out)\n",
    "```\n",
    "![image.png](imagefiltering_files/att_00003.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_torch_channels(tensor, dim = 1, *kwargs):\n",
    "    num_ch = tensor.size(dim)\n",
    "    fig=plt.figure(figsize=(num_ch*5,5))\n",
    "    tensor_splitted = torch.split(tensor, 1, dim=dim)\n",
    "    for i in range(num_ch):\n",
    "        fig.add_subplot(1, num_ch, i+1)\n",
    "        plt.imshow(kornia.tensor_to_image(tensor_splitted[i].squeeze(dim)), *kwargs)\n",
    "    return\n",
    "\n",
    "from imagefiltering import spatial_gradient_first_order\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,15,15] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "sigma = 3.0\n",
    "out = spatial_gradient_first_order(inp, sigma)\n",
    "print (out.shape)\n",
    "imshow_torch_channels(out, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "\n",
    "```python \n",
    "from lab0_reference.imagefiltering import spatial_gradient_first_order\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,15,15] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "sigma = 3.0\n",
    "out = spatial_gradient_first_order(inp, sigma)\n",
    "print (out.shape)\n",
    "imshow_torch_channels(out, 2)\n",
    "```\n",
    "![image.png](imagefiltering_files/att_00004.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagefiltering import spatial_gradient_second_order\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,15,15] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "sigma = 3.0\n",
    "out = spatial_gradient_second_order(inp, sigma)\n",
    "print (out.shape)\n",
    "imshow_torch_channels(out, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from lab0_reference.imagefiltering import spatial_gradient_second_order\n",
    "inp = torch.zeros((1,1,32,32))\n",
    "inp[...,15,15] = 1.\n",
    "imshow_torch(inp)\n",
    "\n",
    "sigma = 3.0\n",
    "out = spatial_gradient_second_order(inp, sigma)\n",
    "print (out.shape)\n",
    "imshow_torch_channels(out, 2)\n",
    "```\n",
    "![image.png](imagefiltering_files/att_00005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting (center, unit_x, unit_y) into affine transform A\n",
    "![image.png](imagefiltering_files/att_00006.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagefiltering import affine\n",
    "inp = torch.tensor([[3, 3.]]), torch.tensor([[6, 3.]]), torch.tensor([[3, 6.]])\n",
    "A = affine(*inp)\n",
    "print (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python \n",
    "from lab0_reference.imagefiltering import affine\n",
    "inp = 3, 3, 6, 3, 3, 6\n",
    "A = affine(*inp)\n",
    "print (A)\n",
    "```\n",
    "\n",
    "    tensor([[3., 0., 3.],\n",
    "            [0., 3., 3.],\n",
    "            [0., 0., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine patch extraction\n",
    "![image.png](imagefiltering_files/att_00007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In function `visualize_A` below, the line shows the Y (to the down) direction of the resulting patch. Remember, that typical image has left-hand coordinate system, where X increases to the right and Y increases to the south (down). Same is true for the extracted patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagefiltering import affine\n",
    "import cv2\n",
    "img1 = cv2.imread('graffiti.ppm')\n",
    "\n",
    "def visualize_A(img, A, **kwargs):\n",
    "    from kornia_moons.feature import visualize_LAF\n",
    "    from kornia.feature import scale_laf\n",
    "    LAF = scale_laf(A[None][:,:,:2], 2.0)\n",
    "    visualize_LAF(img, LAF, **kwargs)\n",
    "    return\n",
    "\n",
    "timg1 = kornia.image_to_tensor(img1, False).float() / 255.\n",
    "timg1 = kornia.color.bgr_to_rgb(timg1)\n",
    "\n",
    "patch_centers = torch.tensor([[300., 200.], [400., 300], [600,600], [100,100]])\n",
    "patch_unitx = torch.tensor([[350., 210.], [450., 300], [650,600], [150,150]])\n",
    "patch_unity = torch.tensor([[270., 150.], [400., 360], [600,650], [50, 150]])\n",
    "\n",
    "A = affine(patch_centers, patch_unitx, patch_unity)\n",
    "print (f'A = {A}')\n",
    "visualize_A(timg1, A, color = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from imagefiltering import extract_affine_patches, affine\n",
    "import cv2\n",
    "img1 = cv2.imread('graffiti.ppm')\n",
    "\n",
    "def visualize_A(img, A, **kwargs):\n",
    "    from kornia_moons.feature import visualize_LAF\n",
    "    from kornia.feature import scale_laf\n",
    "    LAF = scale_laf(A[None][:,:,:2], 2.0)\n",
    "    visualize_LAF(img, LAF, **kwargs)\n",
    "    return\n",
    "\n",
    "timg1 = kornia.image_to_tensor(img1, False).float() / 255.\n",
    "timg1 = kornia.color.bgr_to_rgb(timg1)\n",
    "\n",
    "patch_centers = torch.tensor([[300., 200.], [400., 300], [600,600], [100,100]])\n",
    "patch_unitx = torch.tensor([[350., 210.], [450., 300], [650,600], [150,150]])\n",
    "patch_unity = torch.tensor([[270., 150.], [400., 360], [600,650], [50, 150]])\n",
    "\n",
    "A = affine(patch_centers, patch_unitx, patch_unity)\n",
    "print (f'A = {A}')\n",
    "visualize_A(timg1, A, color = 'blue')\n",
    "```\n",
    "\n",
    "    A = tensor([[[ 50., -30., 300.],\n",
    "         [ 10., -50., 200.],\n",
    "         [  0.,   0.,   1.]],\n",
    "\n",
    "        [[ 50.,   0., 400.],\n",
    "         [  0.,  60., 300.],\n",
    "         [  0.,   0.,   1.]],\n",
    "\n",
    "        [[ 50.,   0., 600.],\n",
    "         [  0.,  50., 600.],\n",
    "         [  0.,   0.,   1.]],\n",
    "\n",
    "        [[ 50., -50., 100.],\n",
    "         [ 50.,  50., 100.],\n",
    "         [  0.,   0.,   1.]]])\n",
    "\n",
    "    patches.shape = torch.Size([4, 3, 32, 32])\n",
    "\n",
    "\n",
    "![image.png](imagefiltering_files/lafs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extract_affine_patches` is function, which you should implement. `extract_antializased_affine_patches` is the function, which calls your function on the appropriate level of the scale pyramid, in order to perform basic level of [anti-aliasing](https://en.wikipedia.org/wiki/Anti-aliasing_filter). \n",
    "See in detail in post \"[Patch extraction: devil in details](https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/22/patch-extraction.html)\"\n",
    "\n",
    "It is recommended to use `extract_antializased_affine_patches` in the tournames and in general to remember about aliasing effects in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from imagefiltering import extract_affine_patches, extract_antializased_affine_patches\n",
    "\n",
    "patches = extract_affine_patches(timg1,\n",
    "                                 A, \n",
    "                                 torch.zeros(A.size(0)).long(),\n",
    "                                 32, 1.0)\n",
    "\n",
    "print (f'patches.shape = {patches.shape}')\n",
    "imshow_torch_channels(patches, 0)\n",
    "\n",
    "patches_AA = extract_antializased_affine_patches(timg1,\n",
    "                                 A, \n",
    "                                 torch.zeros(A.size(0)).long(),\n",
    "                                 32, 1.0)\n",
    "imshow_torch_channels(patches_AA, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from imagefiltering import extract_affine_patches, extract_antializased_affine_patches\n",
    "\n",
    "patches = extract_affine_patches(timg1,\n",
    "                                 A, \n",
    "                                 torch.zeros(A.size(0)).long(),\n",
    "                                 32, 1.0)\n",
    "\n",
    "print (f'patches.shape = {patches.shape}')\n",
    "imshow_torch_channels(patches, 0)\n",
    "\n",
    "patches_AA = extract_antializased_affine_patches(timg1,\n",
    "                                 A, \n",
    "                                 torch.zeros(A.size(0)).long(),\n",
    "                                 32, 1.0)\n",
    "imshow_torch_channels(patches_AA, 0)\n",
    "```\n",
    "\n",
    "![image.png](imagefiltering_files/patches.png)\n",
    "![image.png](imagefiltering_files/patches_AA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagefiltering import affine\n",
    "import cv2\n",
    "import kornia as K\n",
    "import torch\n",
    "img1 = cv2.cvtColor(cv2.imread('graffiti.ppm'), cv2.COLOR_BGR2RGB).astype(np.float32)/255.\n",
    "\n",
    "\n",
    "def there_and_back(img, blur=False, angle=45.):\n",
    "    h,w = img.shape[:2]\n",
    "    timg = K.image_to_tensor(img, False).float()\n",
    "    if blur:\n",
    "        timg = K.filters.gaussian_blur2d(timg, (15,15), (4.0, 4.0))\n",
    "    img_rot = K.geometry.rotate(timg, torch.tensor(angle))\n",
    "    img_down = K.geometry.resize(img_rot, (h//7, w//7))\n",
    "    img_rot_back = K.geometry.rotate(img_down, torch.tensor(-angle))\n",
    "    img_out = K.geometry.resize(img_rot_back, (h,w))\n",
    "    return K.tensor_to_image(img_out)\n",
    "img_rot = there_and_back(img1)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(img_rot)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(there_and_back(img1, True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}