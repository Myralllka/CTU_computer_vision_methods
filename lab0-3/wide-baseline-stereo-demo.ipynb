{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](wide-baseline-stereo-demo_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the first part of the course we implement from scratch complete wide baseline stereo pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first do in in OpenCV to get the high-level understanding of the steps. We have two image as an input and would like to get corresponces and perspective transform between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "#Images are taken from HSequences dataset https://github.com/hpatches/hpatches-dataset\n",
    "img1 = cv2.cvtColor(cv2.imread('v_woman1.ppm'), cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread('v_woman6.ppm'), cv2.COLOR_BGR2RGB)\n",
    "H_gt = np.loadtxt('v_woman_H_1_6')\n",
    "\n",
    "def show_two_images(img1, img2):\n",
    "    fig=plt.figure(figsize=(20,10))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img1)\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(img2)\n",
    "    return\n",
    "\n",
    "show_two_images(img1, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labs 1 and 2: local feature detection and description\n",
    "\n",
    "![image.png](wide-baseline-stereo-demo_files/att_00001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = cv2.AKAZE_create()    \n",
    "\n",
    "kps1, descs1 = det.detectAndCompute(img1,None)\n",
    "kps2, descs2 = det.detectAndCompute(img2,None)\n",
    "\n",
    "\n",
    "vis_img1, vis_img2 = None,None\n",
    "vis_img1 = cv2.drawKeypoints(cv2.cvtColor(img1,cv2.COLOR_RGB2GRAY),kps1,vis_img1, \n",
    "                             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "vis_img2 = cv2.drawKeypoints(cv2.cvtColor(img2,cv2.COLOR_RGB2GRAY),kps2,vis_img2, \n",
    "                             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "show_two_images(vis_img1, vis_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3: getting tentative correspondence\n",
    "![image.png](wide-baseline-stereo-demo_files/att_00002.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_descriptors(descriptors1, descriptors2):\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMING)\n",
    "    knn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n",
    "\n",
    "    ratio_thresh = 0.8\n",
    "    tentative_matches = []\n",
    "    for m,n in knn_matches:\n",
    "        if m.distance < ratio_thresh * n.distance:\n",
    "            tentative_matches.append(m)\n",
    "    return tentative_matches\n",
    "\n",
    "def decolorize(img):\n",
    "    return  cv2.cvtColor(cv2.cvtColor(img,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "tentative_matches = match_descriptors(descs1, descs2)\n",
    "img_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "cv2.drawMatches(decolorize(img1), kps1, decolorize(img2), kps2, tentative_matches, img_matches, \n",
    "                flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3: RANSAC\n",
    "\n",
    "![image.png](wide-baseline-stereo-demo_files/att_00003.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_matches(kps1, kps2, tentative_matches, H,  H_gt, inlier_mask, img1, img2):\n",
    "    matchesMask = inlier_mask.ravel().tolist()\n",
    "    h,w,ch = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    #Ground truth transformation\n",
    "    dst_GT = cv2.perspectiveTransform(pts, H_gt)\n",
    "    img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)\n",
    "    img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "    # Blue is estimated, green is ground truth homography\n",
    "    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 20)\n",
    "    img_out = cv2.drawMatches(decolorize(img1),kps1,img2_tr,kps2,tentative_matches,None,**draw_params)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img_out)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "\n",
    "tentative_matches_torch = torch.cat([torch.from_numpy(src_pts), \n",
    "                                     torch.from_numpy(dst_pts)], dim=1)\n",
    "print (tentative_matches_torch.shape)\n",
    "torch.save(tentative_matches_torch, 'v_woman1-6_tentatives.pth')\n",
    "load_matches = torch.load('v_woman1-6_tentatives.pth')\n",
    "print (load_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometric verification (RANSAC)\n",
    "from copy import deepcopy\n",
    "def verify(tentative_matches, kps1, kps2):\n",
    "    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,1.0)\n",
    "    return H, inlier_mask\n",
    "    \n",
    "\n",
    "H, inliers =  verify(tentative_matches, kps1, kps2)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches, H, H_gt, inliers, img1, img2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}