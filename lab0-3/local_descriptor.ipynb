{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Local feature description\n",
    "This is a notebook, which could help you with testing third lab assignment.\n",
    "It contains utility functions for visualization, some test input for the functions you needs to implement,\n",
    "and the output of the reference solution for the same test input.\n",
    "\n",
    "template functions for the assignment contain a short description of what the function is supposed to do,\n",
    "and produce an incorrect output, which is nevertheless in proper format: type and shape.\n",
    "\n",
    "You are not allowed to use kornia or opencv or any other library functions, which are specifically designed\n",
    "to perform the operations requested in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import kornia as K\n",
    "\n",
    "\n",
    "def plot_torch(x, y, *kwargs):\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy(), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch(tensor,figsize=(8,6), *kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(kornia.tensor_to_image(tensor), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch_channels(tensor, dim = 1, *kwargs):\n",
    "    num_ch = tensor.size(dim)\n",
    "    fig=plt.figure(figsize=(num_ch*5,5))\n",
    "    tensor_splitted = torch.split(tensor, 1, dim=dim)\n",
    "    for i in range(num_ch):\n",
    "        fig.add_subplot(1, num_ch, i+1)\n",
    "        plt.imshow(kornia.tensor_to_image(tensor_splitted[i].squeeze(dim)), *kwargs)\n",
    "    return\n",
    "\n",
    "def timg_load(fname, to_gray = True):\n",
    "    img = cv2.imread(fname)\n",
    "    with torch.no_grad():\n",
    "        timg = kornia.image_to_tensor(img, False).float()\n",
    "        if to_gray:\n",
    "            timg = kornia.color.bgr_to_grayscale(timg)\n",
    "        else:\n",
    "            timg = kornia.color.bgr_to_rgb(timg)\n",
    "    return timg\n",
    "\n",
    "\n",
    "def visualize_detections(img, keypoint_locations, img_idx = 0, increase_scale = 1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4].item(),\n",
    "                         b_ch_sc_y_x[3].item(),\n",
    "                         increase_scale * b_ch_sc_y_x[2].item(), 90)\n",
    "            for b_ch_sc_y_x in keypoint_locations if b_ch_sc_y_x[0].item() == img_idx]\n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(kornia.tensor_to_image(img).astype(np.uint8),\n",
    "                                kpts,\n",
    "                                vis_img, \n",
    "                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(vis_img)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load an image to work with and detect some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timg = timg_load('graffiti.ppm', False)/255.\n",
    "timg_gray = kornia.color.rgb_to_grayscale(timg)\n",
    "imshow_torch(timg)\n",
    "\n",
    "from local_detector import *\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations = scalespace_hessian(timg_gray, 0.001)\n",
    "    visualize_detections(timg_gray*255., keypoint_locations, increase_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_descriptor import *\n",
    "\n",
    "A, img_idxs = affine_from_location(keypoint_locations)\n",
    "\n",
    "print (f'keypoint_locations={keypoint_locations[100]}')\n",
    "print (f'A={A[100]}')\n",
    "\n",
    "idxs = range(100,105)\n",
    "visualize_detections(timg_gray*255., keypoint_locations[idxs], increase_scale=6.0) \n",
    "patches = extract_affine_patches(timg, A[idxs], img_idxs[idxs], 32, 6.0/2.)\n",
    "# OpenCV visualization treats keypoint scale as diameter, while extract_affine_patches as radius,\n",
    "# therefore we should divide by two to get same area\n",
    "\n",
    "imshow_torch_channels(patches, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "```python\n",
    "from local_descriptor import *\n",
    "\n",
    "A, img_idxs = affine_from_location(keypoint_locations)\n",
    "\n",
    "print (f'keypoint_locations={keypoint_locations[1000]}')\n",
    "print (f'A={A[1000]}')\n",
    "\n",
    "idxs = range(1000,1005)\n",
    "visualize_detections(timg_gray*255., keypoint_locations[idxs], increase_scale=6.0) \n",
    "patches = extract_affine_patches(timg, A[idxs], img_idxs[idxs], 32, 6.0/2.)\n",
    "# OpenCV visualization treats keypoint scale as diameter, while extract_affine_patches as radius,\n",
    "# therefore we should divide by two to get same area\n",
    "\n",
    "imshow_torch_channels(patches, 0)\n",
    "```\n",
    "\n",
    "![image.png](local_descriptor_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_descriptor import *\n",
    "\n",
    "input_kp_locations = torch.tensor([0, 0, 10., 150, 340]).view(1, 5)\n",
    "input_orientation = torch.tensor([math.pi / 2.0]).view(1,1)\n",
    "input_affine_shape = torch.tensor([4., 0., 2.]).view(1,3)\n",
    "\n",
    "A, img_idxs  = affine_from_location(input_kp_locations)\n",
    "print (A.long())\n",
    "A1, img_idxs = affine_from_location_and_orientation(input_kp_locations, input_orientation)\n",
    "print (A1.long())\n",
    "A2, img_idxs = affine_from_location_and_orientation_and_affshape(input_kp_locations, \n",
    "                                                                 input_orientation,\n",
    "                                                                 input_affine_shape)\n",
    "print (A2.long())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refence example\n",
    "\n",
    "```python\n",
    "from local_descriptor import *\n",
    "\n",
    "input_kp_locations = torch.tensor([0, 0, 10., 150, 340]).view(1, 5)\n",
    "input_orientation = torch.tensor([math.pi / 2.0]).view(1,1)\n",
    "input_affine_shape = torch.tensor([4., 0., 2.]).view(1,3)\n",
    "\n",
    "A, img_idxs  = affine_from_location(input_kp_locations)\n",
    "print (A.long())\n",
    "A1, img_idxs = affine_from_location_and_orientation(input_kp_locations, input_orientation)\n",
    "print (A1.long())\n",
    "A2, img_idxs = affine_from_location_and_orientation_and_affshape(input_kp_locations, \n",
    "                                                                 input_orientation,\n",
    "                                                                 input_affine_shape)\n",
    "print (A2.long())\n",
    "```\n",
    "    tensor([[[ 10,   0, 340],\n",
    "             [  0,  10, 150],\n",
    "             [  0,   0,   1]]])\n",
    "    tensor([[[  0,  10, 340],\n",
    "             [-10,   0, 150],\n",
    "             [  0,   0,   1]]])\n",
    "    tensor([[[  0,   8, 340],\n",
    "             [-11,   0, 150],\n",
    "             [  0,   0,   1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local patch orientation\n",
    "\n",
    "Script for benchmarking local patch orientation, similar to one in AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_descriptor import estimate_patch_dominant_orientation \n",
    "def normalize_angle(ang):\n",
    "    #https://stackoverflow.com/a/22949941/1983544\n",
    "    return ang - (torch.floor((ang + K.pi)/(2.0*K.pi)))*2.0*K.pi;\n",
    "\n",
    "def benchmark_orientation_consistency(orienter, patches, PS_out, angles=[15], bins=36):\n",
    "    import kornia as K\n",
    "    kornia_major_version = float(K.version.__version__.split('.')[0]) +0.1*float(K.version.__version__.split('.')[1])\n",
    "    from kornia import center_crop\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        patches_orig_crop = center_crop(patches, (PS_out, PS_out))\n",
    "        ang_out = normalize_angle(orienter(patches_orig_crop, bins))     \n",
    "        for ang_gt in angles:\n",
    "            ang_gt = torch.tensor(ang_gt)\n",
    "            patches_ang = K.rotate(patches, ang_gt)\n",
    "            patches_ang_crop = center_crop(patches_ang, (PS_out, PS_out))\n",
    "            ang_out_ang = normalize_angle(orienter(patches_ang_crop, bins))\n",
    "            error_aug_cw = normalize_angle(ang_out - K.deg2rad(ang_gt) - ang_out_ang).abs()\n",
    "            error_aug_ccw = normalize_angle(ang_out + K.deg2rad(ang_gt) - ang_out_ang).abs()\n",
    "            if error_aug_ccw.mean().item() < error_aug_cw.mean().item():\n",
    "                error_aug = error_aug_ccw\n",
    "            else:\n",
    "                error_aug = error_aug_cw\n",
    "            errors.append(error_aug.mean())\n",
    "            print (f'mean consistency error = {K.rad2deg(error_aug.mean()):.1f} [deg]') \n",
    "    return K.rad2deg(torch.stack(errors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load patches and check. Threshold for passing the test is 20 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_fname = 'patches/'\n",
    "fnames = os.listdir(dir_fname)\n",
    "angles = [90., 60., 45., 30.]\n",
    "PS_out = 32\n",
    "PS = 65\n",
    "\n",
    "\n",
    "angles = [70., 30.]\n",
    "orienter = estimate_patch_dominant_orientation\n",
    "with torch.no_grad():\n",
    "    errors = []\n",
    "    for f in fnames[::-1]:\n",
    "        fname = os.path.join(dir_fname, f)\n",
    "        patches = K.image_to_tensor(np.array(Image.open(fname).convert(\"L\"))).float() / 255.\n",
    "        patches = patches.reshape(-1, 1, PS, PS)\n",
    "        err = benchmark_orientation_consistency(orienter, patches, PS_out, angles)\n",
    "        errors.append(err)\n",
    "    AVG_ERR = torch.stack(errors).mean().item()\n",
    "    print (f'Average error = {AVG_ERR:.1f} deg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refence example\n",
    "\n",
    "```python\n",
    "dir_fname = 'patches/'\n",
    "fnames = os.listdir(dir_fname)\n",
    "angles = [90., 60., 45., 30.]\n",
    "PS_out = 32\n",
    "PS = 65\n",
    "import sys\n",
    "sys.path.insert(0,'/home/old-ufo/dev/BRUTE/mpv-test-04/upload_system/')\n",
    "from refence_solution.local_descriptor import estimate_patch_dominant_orientation as REF\n",
    "\n",
    "\n",
    "angles = [70., 30.]\n",
    "orienter = REF\n",
    "with torch.no_grad():\n",
    "    errors = []\n",
    "    for f in fnames[::-1]:\n",
    "        fname = os.path.join(dir_fname, f)\n",
    "        patches = K.image_to_tensor(np.array(Image.open(fname).convert(\"L\"))).float() / 255.\n",
    "        patches = patches.reshape(-1, 1, PS, PS)\n",
    "        err = benchmark_orientation_consistency(orienter, patches, PS_out, angles)\n",
    "        errors.append(err)\n",
    "    AVG_ERR = torch.stack(errors).mean().item()\n",
    "    print (f'Average error = {AVG_ERR:.1f} deg')\n",
    "```\n",
    "\n",
    "    mean consistency error = 1.9 [deg]\n",
    "    mean consistency error = 3.0 [deg]\n",
    "    Average error = 2.4 deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}